# 海量数据的一些算法

最近有个黑洞照片的事情(2019-04-10)，2017年照片就拍好了，但是照片合成却花了两年，就是因为数据量太大且数据分散在世界各地，2017年8个望远镜的数据量就达到了10240TB，世界各地的数据光运到处理中心就花了几个月，这才是在真的海量数据。

**插播一个对数据量进行转换的操作**

在计算机里面，按照标准的数据进行转换

* 1 GB = 1024 MB
* 1 MB = 1024 KB
* 1 KB = 1024 Byte
* 1 Byte = 8 Bit

但是在估算的时候，往往不采用这些1024，而是将其约等于1000。这样

* 1 GB = 1000 MB
* 1 MB = 1000 KB
* 1 KB = 1000 Byte
* 1 Byte = 8 Bit



比如估计10亿个数需要存储使用多少空间的时候，是这么算的。

10 亿个整数 = 4 * 10 * 10^8 个字节 = 4 * 10^9 字节 = 4GB

**一般涉及到的海量数据的面试题都有两个提示**

1. 数据量很大，比如10^8也即是1亿个整数，要完全存储需要4GB内存
2. 硬件条件有限，比如内存很小，只有1GB

一般遇见海量数据，就需要布隆过滤器出手，或者利用情景特点进行数据拆分。

## 布隆过滤器

**布隆过滤器的特点是，宁可错杀一千，绝不放过一个。意思就是，布隆过滤器说你不在黑名单里面，你就肯定不在黑名单里面，说你在黑名单里面，有可能冤枉了你**

* 网页爬虫对URL进行去重，避免爬去相同的URL地址
* 进行垃圾邮件（短信）的过滤，判断一个发信人是不是在垃圾黑名单中
* 应对黑客对数据库的缓存攻击。有的黑客为了让服务器宕机，会构建大量的不存在的key向数据库进行请求，这个时候数据库的缓存就不够用了，就需要对数据库进行频繁的查询（缓存击穿），在数据量大的情况下，是有可能把数据库整挂的。布隆过滤器就可以封掉那些常发送不在数据库缓存中的IP请求。

## 对未排序的海量数据查找中位数
